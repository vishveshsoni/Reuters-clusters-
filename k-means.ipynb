{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAHIA COCOA REVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STANDARD OIL &lt;SRD&gt; TO FORM FINANCIAL UNIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEXAS COMMERCE BANCSHARES &lt;TCB&gt; FILES PLAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TALKING POINT/BANKAMERICA &lt;BAC&gt; EQUITY OFFER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title_text\n",
       "0                                BAHIA COCOA REVIEW\n",
       "1         STANDARD OIL <SRD> TO FORM FINANCIAL UNIT\n",
       "2        TEXAS COMMERCE BANCSHARES <TCB> FILES PLAN\n",
       "3      TALKING POINT/BANKAMERICA <BAC> EQUITY OFFER\n",
       "4  NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"title_dataset.csv\",error_bad_lines=False,usecols =[\"title_text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>Bundesbank says it leaves credit policies unc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>Bundesbank says it leaves credit policies unc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>Bundesbank sets 28-day securities repurchase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13258</th>\n",
       "      <td>Bundesbank sets 28-day securities repurchase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>&lt;ACC CORP&gt; 3RD QTR NET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20193</th>\n",
       "      <td>&lt;ACC CORP&gt; 3RD QTR NET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>&lt;FRANKLIN CALIFORNIA TAX-FREE INCOME FUND&gt;PAYOUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19061</th>\n",
       "      <td>&lt;FRANKLIN CALIFORNIA TAX-FREE INCOME FUND&gt;PAYOUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title_text\n",
       "1948    Bundesbank says it leaves credit policies unc...\n",
       "6989    Bundesbank says it leaves credit policies unc...\n",
       "8100    Bundesbank sets 28-day securities repurchase ...\n",
       "13258   Bundesbank sets 28-day securities repurchase ...\n",
       "20476                             <ACC CORP> 3RD QTR NET\n",
       "20193                             <ACC CORP> 3RD QTR NET\n",
       "499     <FRANKLIN CALIFORNIA TAX-FREE INCOME FUND>PAYOUT\n",
       "19061   <FRANKLIN CALIFORNIA TAX-FREE INCOME FUND>PAYOUT"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['title_text'].duplicated(keep=False)].sort_values('title_text').head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates('title_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def tokenize_and_stem(text, do_stem=True):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    # stem filtered tokens\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    \n",
    "    if do_stem:\n",
    "        return stems\n",
    "    else:\n",
    "        return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in data['title_text']:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_and_stem(i, False)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "eng_contractions = [\"ain't\", \"amn't\", \"aren't\", \"can't\", \"could've\", \"couldn't\",\n",
    "                    \"daresn't\", \"didn't\", \"doesn't\", \"don't\", \"gonna\", \"gotta\", \n",
    "                    \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\",\n",
    "                    \"how'll\", \"how's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\",\n",
    "                    \"it'll\", \"it's\", \"let's\", \"mayn't\", \"may've\", \"mightn't\", \n",
    "                    \"might've\", \"mustn't\", \"must've\", \"needn't\", \"o'clock\", \"ol'\",\n",
    "                    \"oughtn't\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\",\n",
    "                    \"shouldn't\", \"somebody's\", \"someone's\", \"something's\", \"that'll\",\n",
    "                    \"that're\", \"that's\", \"that'd\", \"there'd\", \"there're\", \"there's\", \n",
    "                    \"these're\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this's\",\n",
    "                    \"those're\", \"tis\", \"twas\", \"twasn't\", \"wasn't\", \"we'd\", \"we'd've\",\n",
    "                    \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'd\", \"what'll\", \n",
    "                    \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where're\",\n",
    "                    \"where's\", \"where've\", \"which's\", \"who'd\", \"who'd've\", \"who'll\",\n",
    "                    \"who're\", \"who's\", \"who've\", \"why'd\", \"why're\", \"why's\", \"won't\",\n",
    "                    \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \n",
    "                    \"you've\", \"'s\", \"s\"\n",
    "                     ]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "custom_stopwords = text.ENGLISH_STOP_WORDS.union(eng_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bahia</th>\n",
       "      <td>bahia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cocoa</th>\n",
       "      <td>cocoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>oil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words\n",
       "bahia        bahia\n",
       "cocoa        cocoa\n",
       "review      review\n",
       "standard  standard\n",
       "oil            oil"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'m\", 'abov', 'afterward', 'ai', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'ca', 'cri', 'dare', 'describ', 'did', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'gon', 'got', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'let', 'll', 'mani', 'meanwhil', 'moreov', \"n't\", 'na', 'need', 'nobodi', 'noon', 'noth', 'nowher', 'ol', 'onc', 'onli', 'otherwis', 'ought', 'ourselv', 'perhap', 'pleas', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'ta', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 've', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20029, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=200000,\n",
    "                                 min_df=0.1, stop_words=custom_stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['title_text']) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['title_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'m\", 'abov', 'afterward', 'ai', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'ca', 'cri', 'dare', 'describ', 'did', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'gon', 'got', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'let', 'll', 'mani', 'meanwhil', 'moreov', \"n't\", 'na', 'need', 'nobodi', 'noon', 'noth', 'nowher', 'ol', 'onc', 'onli', 'otherwis', 'ought', 'ourselv', 'perhap', 'pleas', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'ta', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 've', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, max_features=200000,\n",
    "                                 min_df=0.0, stop_words=custom_stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix_test = tfidf_vectorizer.fit_transform(['BAHIA COCOA REVIEW']) #fit the vectorizer to synopses\n",
    "\n",
    "# print(tfidf_matrix.shape)\n",
    "\n",
    "# terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:970: FutureWarning: 'precompute_distances' was deprecated in version 0.23 and will be removed in 0.25. It has no effect\n",
      "  \"effect\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:974: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(init='random', max_iter=1000, n_clusters=10, n_init=30, n_jobs=1,\n",
       "       precompute_distances='auto', random_state=10)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "\n",
    "num_clusters = int(math.sqrt(data.shape[0] / 2) * 1.5)\n",
    "\n",
    "km = KMeans(algorithm='auto', copy_x=True, init='random', max_iter=1000,\n",
    "    n_clusters=10, n_init=30, n_jobs=1, precompute_distances='auto',\n",
    "    random_state=10, tol=0.0001, verbose=0)\n",
    "\n",
    "km.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km.labels_.tolist()\n",
    "\n",
    "data['cluster'] = clusters\n",
    "data.to_csv(\"title_dataset_out.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: qtr,\n",
      "\n",
      "Cluster 0 titles:\n",
      "17925\n",
      "\n",
      "\n",
      "Cluster 1 words: qtr,\n",
      "\n",
      "Cluster 1 titles:\n",
      "2104\n",
      "\n",
      "\n",
      "Cluster 2 words: qtr,\n",
      "\n",
      "Cluster 2 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 3 words: qtr,\n",
      "\n",
      "Cluster 3 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 4 words: qtr,\n",
      "\n",
      "Cluster 4 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 5 words: qtr,\n",
      "\n",
      "Cluster 5 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 6 words: qtr,\n",
      "\n",
      "Cluster 6 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 7 words: qtr,\n",
      "\n",
      "Cluster 7 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 8 words: qtr,\n",
      "\n",
      "Cluster 8 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 9 words: qtr,\n",
      "\n",
      "Cluster 9 titles:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    print()\n",
    "    print(len(data[data['cluster'] == i]['title_text'].values.tolist()))\n",
    "#         print(' - %s' % title)\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = km.predict(tfidf_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m52",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m52"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
